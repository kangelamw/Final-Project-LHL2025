{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from functions import *\n",
    "\n",
    "# Paths\n",
    "save_data = '../data'\n",
    "data_path_train = os.path.join(save_data, 'train.csv')\n",
    "data_path_test = os.path.join(save_data, 'test.csv')\n",
    "\n",
    "save_images = '../images'\n",
    "save_model = '../model'\n",
    "\n",
    "if os.path.exists(save_data):\n",
    "  print('Data: Save point initialized.')\n",
    "if os.path.exists(data_path_train):\n",
    "  print(f'Train loaded')\n",
    "if os.path.exists(data_path_test):\n",
    "  print(f'Test loaded\\n')\n",
    "\n",
    "if os.path.exists(save_images):\n",
    "  print('Images: Save point initialized.')\n",
    "if os.path.exists(save_model):\n",
    "  print('Model: Save point initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More imports\n",
    "import torch\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, Training\n",
    "from datasets import Dataset\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_train = pd.read_csv(data_path_train)\n",
    "df_test = pd.read_csv(data_path_test)\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Datasets\n",
    "train = Dataset.from_pandas(df_train)\n",
    "test = Dataset.from_pandas(df_test)\n",
    "\n",
    "train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                           num_labels=3,\n",
    "                                                           device_map={\"\": 0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Get model device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n=== === === Model loaded on: {device} === === ===\\n\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], \n",
    "                     padding=\"max_length\", \n",
    "                     truncation=True)\n",
    "\n",
    "# Tokenize the dataset\n",
    "train = train.map(tokenize_function, batched=True)\n",
    "test = test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert labels to torch tensors\n",
    "train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metrics\n",
    "import evaluate\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Load evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "cross_entropy = evaluate.load(\"log_loss\")  # Approximate Cross-Entropy Loss\n",
    "\n",
    "# Function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = softmax(logits, axis=-1)  # logits to probs\n",
    "    predictions = np.argmax(probs, axis=-1)  # Get class preds\n",
    "\n",
    "    labels_one_hot = np.eye(probs.shape[-1])[labels]  # kl divergence requires one-hot labels\n",
    "    kl_divs = [entropy(labels_one_hot[i], probs[i]) for i in range(len(labels))]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
    "        \"cross_entropy\": cross_entropy.compute(predictions=probs, references=labels)[\"log_loss\"],\n",
    "        \"kl_divergence\": np.mean(kl_divs)  # Average KL-Divergence across all samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(save_model, 'roberta-classifier-bias-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=model_path,\n",
    "  do_train=True,\n",
    "  do_eval=True,\n",
    "  do_predict=True,\n",
    "  \n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=150,\n",
    "  eval_accumulation_steps=4,\n",
    "  \n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=300,\n",
    "  \n",
    "  save_strategy=\"steps\",\n",
    "  save_steps=300,\n",
    "  num_train_epochs=5,\n",
    "  \n",
    "  learning_rate=2e-5,\n",
    "  lr_scheduler_type=\"linear\",\n",
    "  warmup_ratio=0.1,\n",
    "  weight_decay=0.01,\n",
    "  \n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"f1\",\n",
    "  greater_is_better=True,\n",
    "  \n",
    "  report_to=\"tensorboard\",\n",
    "  resume_from_checkpoint=True,\n",
    "  \n",
    "  per_device_eval_batch_size=8,\n",
    "  per_device_train_batch_size=8,\n",
    "  gradient_accumulation_steps=4,\n",
    "  gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train,\n",
    "  eval_dataset=test,\n",
    "  compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(model_path, \n",
    "                                'roberta-political-bias-classifier'))\n",
    "tokenizer.save_pretrained(os.path.join(model_path,\n",
    "                                       'roberta-political-bias-classifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "model_performance = pd.DataFrame(eval_results, index=[\"Value\"]).T\n",
    "\n",
    "print(model_performance.to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
